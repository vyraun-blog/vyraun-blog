<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /> <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script><title>Notes from NIPS 2017 (Part 1)</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@vyraun" /><meta name="twitter:title" content="Notes from NIPS 2017 (Part 1)" /><meta name="twitter:description" content=" This is a summary of the tutorial, “Deep Learning Practice and Trends” by Oriol Vinyals and Scott Reeds at NIPS 2017. It’s a rough draft, I will flesh out the post in the coming weeks."><meta name="description" content=" This is a summary of the tutorial, “Deep Learning Practice and Trends” by Oriol Vinyals and Scott Reeds at NIPS 2017. It’s a rough draft, I will flesh out ..."><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/Notes-from-NIPS-2017-Part-1"><link rel="alternate" type="application/atom+xml" title="ML Inside" href="/feed.xml" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"></head><body><aside class="logo"> <!-- <a href="https://vyraun.github.io/"> <img src="/assets/test.jpeg"> </a> !--></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><style style="text/css"> body { background-image: url("https://s-media-cache-ak0.pinimg.com/originals/31/6c/99/316c994144415be5ebadf75be9eef5dc.jpg"); background-position: 0% 0%; background-repeat: no-repeat; background-attachment: fixed; }</style><article><div class="center"><h1>Notes from NIPS 2017 (Part 1)</h1><!-- <a href="https://github.com/vyraun" target="_blank"><i class="fa fa-github-square" style="font-size:24px" title="GitHub"></i></a> <a href="https://twitter.com/vyraun" target="_blank"><i class="fa fa-twitter-square" style="font-size:24px" title="Twitter"></i></a> <a href="https://in.linkedin.com/in/vraunak" target="_blank"><i class="fa fa-linkedin-square" style="font-size:24px" title="LinkedIn"></i></a> --> <time>December 29, 2017</time></div><div class="divider"></div><blockquote><p>This is a summary of the tutorial, “Deep Learning Practice and Trends” by Oriol Vinyals and Scott Reeds at NIPS 2017. It’s a rough draft, I will flesh out the post in the coming weeks.</p></blockquote><p>I attended the 2017 NIPS Conference held at Long Beach as a Microsoft employee (thanks Microsoft India for sending me!). And it was great attending the full conference with tutorials &amp; workshops (workshops were the best!). I found the first tutorial, “Deep learning Practice and Trends” very well prepared. As the title says, the talk was divided into two parts: Practice, which gave a nice high-level view of the Deep Learning toolbox and Trends, which explained 5 trends in deep learning research.</p><iframe width="560" height="400" src="https://www.youtube.com/embed/YJnddoa8sHk" frameborder="0" allowfullscreen=""></iframe><style> .responsive-wrap iframe{ max-height: 100%;}</style><h2 id="the-deep-learning-toolbox">The Deep Learning Toolbox</h2><p>Deep learning excels at structured data, e.g. images. This part covered the standard architectures/tools that form the building blocks of a deep learning model. The key message was that different neural net architectures need to have the right inductive biases, depending on the input &amp; the task they are operating upon. For Images &amp; Image classification, the key inductive biases are 1. Locality Invariance 2. Translation invariance which are realised in a Convolutional layer, which is a locally connected layer with weight sharing.</p><p>Another key tool is depth, but training is a deep net is not easy: depth can’t be parallelized (unlike convolutions, hence slow training) and there are too many parameters to optimize. So how to build very deep CNNs? The tricks are:</p><ul><li>SGD + Momentum (typical for CNNs)</li><li>BatchNorm after each weight layer (It led to higher accuracy plus faster training for Inception v2)</li><li>Weight Initialisation (Glorot works well for VGG)</li><li>Model Building: Use stacks of 3x3 Conv layers &amp; ResNET: Add residual connections that skip 2-3 weight layers. The effect is that since the gradient skips a few weight layers and does not vanish, a better gradient flow is maintained. Another related architecture is highway network and DenseNet (skip connections between everything and everything, resulting in a dense block which can be treated as just another building block) and UNET architecture.</li></ul><p>Further, moving to text, it is worth recalling that the 2 key ingredients that have brought DL into NLP are 1. Neural embeddings (key insight: vectorizing contexts) and RNN language models, which outperformed other approaches to language modelings, since RNNs have persistent memory i.e. a state variable for arbitrarily large contexts.</p><p>Seq2Seq models (easy to implement in Tensorflow) are an extension to RNNs, however they have the limitation that increasing sentence length causes a bottleneck; which leads to a sharp drop in BLEU scores in the NMT task. Attention is a mechanism to alleviate this information bottleneck. The idea in attention (Bahdanau paper, 2015) is that the decoder can query the encoder at every time step. It is really simple to implement and is a way of differentiable content based memory addressing. There are cool extensions to this model as well (R/W Memory as in Neural Turing Machines, Pointer, Key-Value Memory etc.). Finally, some tricks to train sequence models:</p><ol><li>Long Sequences? Use attention, bigger state and reverse inputs (e.g. in translation)</li><li>Can’t overfit? Bigger hidden state, deep LSTM + Skip Connections</li><li>Overfit? Dropout + Ensembles</li><li>Tuning: decrease the learning rate, Initialize parameters (-Uniform(0.05, 0.05)), clip the gradients.</li></ol><p>Now, the cool part of the talk was the Trends part, which discussed a number of models that are really taking deep learning to new levels.</p><h2 id="trend-1-autoregressive-models">Trend 1: Autoregressive models</h2><p>The idea is quite simple: The joint distribution to be learnt can be written as <script type="math/tex">P(x;theta) = ?</script>, and each factor can be parameterized by theta, which can be shared, as long as the ordering and grouping is consistent (i.e. it doesn’t violate causality). Each factor can be parameterized by a DNN (just like in DQNs, which learned to play Atari games by parametering the Q function with a neural net). So, the key questions are 1. how do you order and group the variables and 2. how do you parameterize them?</p><h4 id="part-1-modeling-raw-waveforms-using-causal-convolutions">Part 1: Modeling Raw Waveforms using Causal Convolutions</h4><p>Key thing is that each output depends only on the input from prior time steps. And if you want to get more context for every prediction, you can use dilation. So, you have causal dilated convolutions. And of course, you can stack them. But more tricks are needed: If you have many many possible values the cross entropy loss has a large memory consumption. So they modeled the loss using a discretized mixture of logistic losses (a mixture of sigmoids). Another question is: how do you speed up the sampling? To sample from these models you have to go from left to right which is an O(N) operation. But you can first distill a student net from a teacher net i.e. pretrain a Wavenet teacher in the usual way and then train the student net (kin of like GANs).</p><h4 id="part-2-modeling-text">Part 2: Modeling Text</h4><p>Self attention: the weights are adaptive and it has a more flexible architecture than convolutions (which have the same kernel and same weights).</p><h4 id="part-3-modeling-images">Part 3: Modeling Images</h4><p>You can do it pixel by pixel or group by group (with conditional independence b/w groups, you lose expressiveness but then you can do parallel sampling in O(logN)).</p><p>So, to summarize Autoregressive models, we had 2 kinds of models:</p><ol><li>Fully Sequential Models: You can factorize per sample or per pixel, this gives you fast scoring but O(N) sampling. You can make conditional independence assumptions, which enable faster sampling.</li><li>Distilled Models: Examples are Wavenet, Parallel NMT. They have O(N) scoring and O(1) sampling.</li></ol><h2 id="trend-2-domain-alignment">Trend 2: Domain Alignment</h2><p>Scott Reed said this was the most promising thing in unsupervised learning. And it absolutely is. And it is the method behind two of my favorite papers in NLP this year: Language Style Transfer and Unsupervised Word Translation. The idea behind this trend is also very simple and the problem is immensely practical.</p><p>So let’s see the problem/model description:</p><ul><li>Input: Input is a set of images with some shared structure but there exists no pairing between the images in one domain and the other (i.e. there is no direct alignment). For text, you could have a text corpora in different languages but you don’t have the matching sentences.</li><li>Architectures: They aren’t fancy but are cleverly hooked up.</li><li>Losses: are wired with a loss so that an alignment emerges between the two domains. So, the losses could be in the latent space (e.g. you want the latent representations to be indistinguishable) or in the raw observation space (e.g. pixel space). Adversarial objectives and max likelihood both work well.</li></ul><h4 id="approach-1-weakly-supervised-eg-visual-domain-alignment-bw-svhn-and-mnist">Approach 1: Weakly Supervised (e.g. Visual Domain Alignment b/w SVHN and MNIST)</h4><p>So you could do the alignment in a weakly supervised way: share layers with different modalities, train them for some downstream task and then neutrons with activations over the same semantic units occur! Then you can do cross-domain retrieval i.e. you can query the model and retrieve the images in other domains.</p><h4 id="approach-2-adversarial-learning">Approach 2: Adversarial Learning</h4><p>Here, you are aligning the domains by construction. You can share the encoder over several domains, so that the features learnt are invariant to a domain. The paper Domain Adversarial Training of NNs explains it very well.</p><p>Examples:</p><ol><li>Unsupervised cross domain Image generation</li><li>Cycle Consistent Loss (Unpaired Image to Image Translation)</li><li>Unsupervised Image to Image translation</li><li>DiscoGAN</li><li>GraspGAN</li><li>Unsupervised NMT</li><li>Unsupervised NMT using Monolingual Corpora only</li></ol><h2 id="trend-3-meta-learning">Trend 3: Meta Learning</h2><p>It is about a loss that models another loss. The idea is to go beyond a train and test paradigm. So, meta-learning adds a twist to the standard paradigm. The task between the training set and the test set changes, so the model has to learn to learn.</p><ul><li>There are three approaches to meta-learning:<ol><li>Model Based: Model is conditioned on a support set.</li><li>Metric Based: Support set is kept in memory (differentiable nearest neighbour).</li><li>Optimization Based</li></ol></li></ul><h2 id="trend-4-deep-learning-over-graphs">Trend 4: Deep Learning Over Graphs</h2><p>Graphs are hard to represent as tensors. There are two main architectures: message passing and (CNNs, RNNs, attention). But again the question to ask is: What is the right inductive bias for graphs. We want our model to be order invariant. So, X = permutation(X). Further, with graphs batching will be quite tricky.</p><h2 id="trend-5-program-induction-with-nns">Trend 5: Program Induction with NNs</h2><p>So you have 3 ways:</p><ol><li>You can think of NN as a program: embed a program as weights of a NN.</li><li>NN that generates source code. (Learning to execute paper by Zaremba et al. Follows under the paradigm of “Apply Seq2Seq to everthing”, Deep Coder, RobustFill: shows that Neural program induction works).</li><li>Probabilistic Programming with NNs.</li></ol><p>To conclude the trends part:</p><ul><li>Deep AR models and CNNs are in production in consumer apps.</li><li>Inductive biases are very useful: CNNs (spatial), RNNs (time) and Graphs (Perm. Invariance).</li><li>Simple tricks like Resnets/skip-connections are very helpful too.</li><li>Adversarial Nets + Unsupervised domain adaptations work well.</li><li>Meta-learning: more and more lifecycle of a model will be the part of the end to end training.</li><li>Program synthesis and deep geometric learning will become very important.</li></ul></article><div class="back"> <a href="/">Back</a></div></main></body></html>