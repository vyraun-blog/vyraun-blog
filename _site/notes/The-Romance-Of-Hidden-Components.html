<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1" /> <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script><title>The Romance of Hidden Components</title><meta name="twitter:card" content="summary" /><meta name="twitter:site" content="@vyraun" /><meta name="twitter:title" content="The Romance of Hidden Components" /><meta name="twitter:description" content=" It is amazing that deep learning works. Billions of parameters simultaneously converging to a good solution, not getting trapped in local minima. In convex optimization, the theory is very clear ..."><meta name="description" content=" It is amazing that deep learning works. Billions of parameters simultaneously converging to a good solution, not getting trapped in local minima. In convex..."><meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o"><link rel="icon" href="/assets/favicon.png"><link rel="apple-touch-icon" href="/assets/touch-icon.png"><link rel="stylesheet" href="//code.cdn.mozilla.net/fonts/fira.css"><link rel="stylesheet" href="/assets/core.css"><link rel="canonical" href="/notes/The-Romance-Of-Hidden-Components"><link rel="alternate" type="application/atom+xml" title="ML Inside" href="/feed.xml" /><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"></head><body><aside class="logo"> <!-- <a href="https://vyraun.github.io/"> <img src="/assets/test.jpeg"> </a> !--></aside><main> <noscript><style> article .footnotes { display: block; }</style></noscript><style style="text/css"> body { background-image: url("https://s-media-cache-ak0.pinimg.com/originals/31/6c/99/316c994144415be5ebadf75be9eef5dc.jpg"); background-position: 0% 0%; background-repeat: no-repeat; background-attachment: fixed; }</style><article><div class="center"><h1>The Romance of Hidden Components</h1><!-- <a href="https://github.com/vyraun" target="_blank"><i class="fa fa-github-square" style="font-size:24px" title="GitHub"></i></a> <a href="https://twitter.com/vyraun" target="_blank"><i class="fa fa-twitter-square" style="font-size:24px" title="Twitter"></i></a> <a href="https://in.linkedin.com/in/vraunak" target="_blank"><i class="fa fa-linkedin-square" style="font-size:24px" title="LinkedIn"></i></a> --> <time>December 31, 2017</time></div><div class="divider"></div><blockquote><p>It is amazing that deep learning works. Billions of parameters simultaneously converging to a good solution, not getting trapped in local minima. In convex optimization, the theory is very clear regarding the convergence of gradient descent (i.e. how many iterations?). But the regime in which deep learning operates is very different, besides just non-convexity. We do not have good intuitions about very high dimensional spaces. Another reason we should understand high-dimensional spaces is that, in the data that we observe, is high dimensionality just nominal? The manifold hypothesis certainly says so. And if the high dimensions are just nominal, how can we find the real manifold where the data resides. This post covers some properties of high-dimensional spaces, how can we extract the real data manifold from a high-dimensional description and some connections to deep neural nets.</p></blockquote><h2 id="properties-of-high-dimensional-spaces">Properties of High Dimensional Spaces</h2><ul><li><p>Why study high-dimensional spaces?</p><p>Well, they are spaces where real world objects reside like images. A 5 MP image (in the pixel space) resides in a 5x10^6 dimensional space. Similarly, tuning neural nets is an optimization problem in a parameter space of billions of parameters.</p></li><li><p>How are they different from low-dimensional spaces?</p><p>There are some fundamental differences. There are curses as well as blessings.</p></li><li><p>It seems impossible to optimise a function with parameters in this space?</p><p>Um.. SGD works well. These spaces are dominated by saddle points. And gradient descent with some noise can escape saddle points. It is an active research area.</p></li><li><p>And what is the true space where the data resides?</p><p>Yes, that’s the crux. If we know where data resides, we can study it, manipulate it. The subject is called manifold learning. PCA is based on certain linear assumptions (data resides in a linear subspace). And then there are non-linear algorithms. And there are neural net approaches too (autoencoders).</p></li></ul><p>… To be continued..</p></article><div class="back"> <a href="/">Back</a></div></main></body></html>